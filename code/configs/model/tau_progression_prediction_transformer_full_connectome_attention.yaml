_target_: src.models.tau_progression_prediction_transformer_full_connectome_attention.TransformerModelFullConnAtt

# Given
d_in: 203
d_out: 200

# Tunable
d_model: 200 # instead of 203, so it's divisible by n_encoder_heads
d_hid: 100
max_len: 10
n_encoder_heads: 4
n_encoder_layers: 2
lr: 0.001
activation: "gelu"
transformer_dropout: 0.2
dropout: 0.2
